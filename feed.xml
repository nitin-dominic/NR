<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://nitin-dominic.github.io/NR/NR/feed.xml" rel="self" type="application/atom+xml"/><link href="https://nitin-dominic.github.io/NR/NR/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-12-02T00:28:32+00:00</updated><id>https://nitin-dominic.github.io/NR/NR/feed.xml</id><title type="html">Nitin Rai</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">The script that helped me win 3MT thesis competition</title><link href="https://nitin-dominic.github.io/NR/NR/blog/2023/3MT-Thesis/" rel="alternate" type="text/html" title="The script that helped me win 3MT thesis competition"/><published>2023-02-16T13:56:00+00:00</published><updated>2023-02-16T13:56:00+00:00</updated><id>https://nitin-dominic.github.io/NR/NR/blog/2023/3MT-Thesis</id><content type="html" xml:base="https://nitin-dominic.github.io/NR/NR/blog/2023/3MT-Thesis/"><![CDATA[<p>After much considerations, I finally plan on writing a small post about the script (summary of my research) that helped me win NDSU‚Äôs annual 3MT Thesis competition. This was my first time (since the time I joined NDSU in the fall 2019) participating in the competition with 2-levels of rounds, preliminary and championship. Below is the script that I wrote and memorized it really hard while trying to speak atleat 5-times a day so that I don‚Äôt forget it when facing a large audience. My presentation was titled, <strong>Drone spots illegal weed plants: The future of agriculture takes flight</strong>. I have provided some strike-throughs (<del>this</del>, with changes made in the brackets) throughout the script that basically portrays how I navigated my script for the final talk after gathering advise from a mentor. Also, the bold fonts (<strong>This one</strong>) mean that an expression or an emphasis was required. üë®‚Äçüî¨</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/NR/assets/img/3MT_Nitin-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/NR/assets/img/3MT_Nitin-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/NR/assets/img/3MT_Nitin-1400.webp"/> <img src="/NR/assets/img/3MT_Nitin.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Dr. Colleen Fitzgerald, Vice President for Research and Creative Activity at North Dakota State University, handing over the prize money to me right after the competition. </div> <p><em>The script starts below‚Ä¶</em></p> <p>Have you ever <strong>wondered</strong> if an uncrewed aircraft system (UAS) or drones could be used to monitor agricultural farms? Well, I carried the <strong>same curiosity</strong> almost 3-and-a-half years back and now my research focuses on the application of drones in agriculture.</p> <p><em>My name is Nitin Rai and I am a Ph.D. candidate in the department of Agricultural and Biosystems Engineering.</em></p> <p>According to a survey, the world population is about to reach <strong>9 billion mark</strong> by the end of 2030. With this population growth rate, growers and farmers <strong>are expected to</strong> increase the use of technolgies that are reliable, fast, and sustainable. One such technologies are drones in agriculture that can be used to monitor weed growth. Now as we all know that weeds are unwanted plants that compete with crop plants for sunlight, water, and nutrients <del>thereby affecting its yield</del> (and what not). For example, a weed by the name of kochia was found to decrease corn production by over 90%. <del>These weeds could be identified by in-field machines and eradicated on-the-go.</del> (<strong>But there‚Äôs a hope!</strong>). Do you know that these weeds could be eliminated by such advanced technologies in real-time?</p> <p>Well allow me to do some time-travelling here <del>by taking you half a century back</del>. In the late 1970s, farmers had to <del>literally</del> walk miles in order to manually <del>apply herbicide solutions</del> (eliminate weeds). <del>Fast forwarded to</del> Another 20 years <del>and engineers developed conventional sprayers</del> (and we have big conventional sprayers). Fast-forwarded to another 20 years and we have drone technology. <strong>So, as a 21st century agricultural engineer, what role do I play in making advancements and contributions to such technologies?</strong> Well, the answer to this is: I teach drones to identify weeds <del>and that‚Äôs where I believe the future of agriculture is headed.</del> (amongst crop plants).</p> <p>So, questioning the gist of my research; how do I teach drones? <del>to spot weeds amongst crop plants?</del> Imagine teaching a 4 year old kid about how an apple looks like? Well you may <del>High chances are that you‚Äôre going to</del> start by describing its textures, such as, an apple is red color, it has a crisp texture, or is roundish in shape. <del>You may also show an image of an apple with different backgrounds and the next time the kid is with you in Walmart, he‚Äôll certainly point out those red apples for a bite!</del> <strong>A computer is just like that 4 year old kid</strong>. In our research we collect <strong>thousands of images of weeds</strong> and <del>train</del> feed it <del>using an approach</del> in an algorithm called as machine learning algorithm. What these machine learning algorithms are capable of <del>doing</del> achieving is extracting <del>very specific features pertaining to</del> relevant features from a specific class of weed specie. <del>We then take these modeled algorithms and deploy them in multiple unseen locations.</del> Once we have these features modeled, we then take these models to multiple field locations to identify those weed species. <del>Once the algorithm identifies the weeds, we compare it with the ground truth data which are labeled by human experts.</del></p> <p>As far as the results <del>from our research is concerned</del> are concerned, our <del>trained</del> algorithm/model has been able to achieve an <strong>overall accuracy of over 85% in identifying 5 different species of weeds across 2 location in the state of ND</strong>. <del>With this accuracy</del> Moreover, our algorithm/model can be integrated with the technologies that are used by growers and farmers <del>would be able to integrate this technology with their spraying</del> to identify and spray weeds in their field as well. technology. <del>Moreover, our algorithm could be trained on small computers in less hours.</del></p> <p>So, does the future of agriculture takes flight <del>by monitoring your agricultural field</del>? <strong>Yes, of course it does!</strong> Therefore, the next time when you are about to eat a delicious meal, <strong>think about these technologies and farmers effort</strong> in quenching your starving tummy.</p> <p>Thank you!!! üòÉ</p> <p>P.S: If you‚Äôre interested in watching a recorded presentation, please click <a href="https://youtu.be/j3DfPBQiBS4">here</a>ü•á.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="bib"/><summary type="html"><![CDATA[After much considerations, I finally plan on writing a small post about the script (summary of my research) that helped me win NDSU‚Äôs annual 3MT Thesis competition. This was my first time (since the time I joined NDSU in the fall 2019) participating in the competition with 2-levels of rounds, preliminary and championship. Below is the script that I wrote and memorized it really hard while trying to speak atleat 5-times a day so that I don‚Äôt forget it when facing a large audience. My presentation was titled, Drone spots illegal weed plants: The future of agriculture takes flight. I have provided some strike-throughs (this, with changes made in the brackets) throughout the script that basically portrays how I navigated my script for the final talk after gathering advise from a mentor. Also, the bold fonts (This one) mean that an expression or an emphasis was required. üë®‚Äçüî¨]]></summary></entry><entry><title type="html">Tennis - Why do I love this sport?</title><link href="https://nitin-dominic.github.io/NR/NR/blog/2022/Tennis/" rel="alternate" type="text/html" title="Tennis - Why do I love this sport?"/><published>2022-12-31T13:56:00+00:00</published><updated>2022-12-31T13:56:00+00:00</updated><id>https://nitin-dominic.github.io/NR/NR/blog/2022/Tennis</id><content type="html" xml:base="https://nitin-dominic.github.io/NR/NR/blog/2022/Tennis/"><![CDATA[<blockquote> <p>There‚Äôs a yellow colored sticky note in my room that says, ‚ÄúSports build character and that‚Äôs why Tennis.‚Äù¬†Period.</p> </blockquote> <p>When I was 12 or 13, I vividly remember asking my dad if he could look for some tennis courts in the city (I did not own an Android phone back then!) and talk with the authorities to get me registered for playing tennis. The city that I was raised in India had only limited number of tennis courts owned by some big Clubs that charged high annual membership fees to allow its members to play tennis. With tennis in my heart, I grew up watching Federer and Nadal play and often used to think that hitting balls the way they did was easy. All you have to do is to be very energetic and active while you‚Äôre on the court. Alas! I WAS WRONG!!!</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/NR/assets/img/tennis-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/NR/assets/img/tennis-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/NR/assets/img/tennis-1400.webp"/> <img src="/NR/assets/img/tennis.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Me trying a backhand while playing on one of the University Village Courts near NDSU-Main Campus. </div> <p>Years passed by and I never got a chance to play tennis until 2017. I got into IIT KGP to pursue my master‚Äôs study and it was for the first time when I saw 5‚Äì6 open tennis courts on-campus. My love for tennis was rejuvenated. But the biggest hurdle was to buy a good racquet and a set of balls to play with (which indeed was expensive!). I was a bit hesitant to ask my dad to sponsor a significant amount just to buy a racquet and set of balls (although I knew he would definitely pay for it!). However, I was able to talk with someone and managed to procure a Wilson racquet. Since then, I have been playing tennis and enjoying the sport more each day (but I am not consistent at all). Over time I have thought that I played many sports including, Volleyball, Cricket (I don‚Äôt like Cricket at all for some reason!), Squash, Badminton, Football (Soccer and not the American Football), Table-Tennis (Ping-pong), and Tennis (of-course). But I love tennis the most and is special to me due to following reasons:</p> <p><strong>Tennis demands mental strength:</strong> I won‚Äôt be bias at all if I say that tennis demands mental strength. That does not mean that the other sports does not. But playing tennis as singles is really exhausting and that too for 5-6 sets straight (think about the professionals!). It demands strong characters of an athelete. Tennis also involves strategic thinking ability to attack your opponent‚Äô weakness + judging the balls. One has to be actively thinking and ‚ÄúBE PRESENT MENTALLY‚Äù on the court while playing this sport.</p> <p><strong>Tennis demands physical strength that leads to hitting strategic shots:</strong> This part is directly linked to the mental strength. If one can think to attack the opponent, one has to act too. In spite of not just simply running on the court, one should have charasmatic reflexes too in order to hit the ball just in the direction they might have thought. It requires a coordination between what one thinks and how one acts. ¬†</p> <p><strong>Fighting for each point</strong>: In a way, tennis has taught me so much about life. No matter how challenging life can get on an every day basis (for points when playing), one always get an opportunity to fight back sometime!¬†One of the major reason when I am on the court, I always have this inner feeling to fight and play the best as per my strength both mentally and physically.</p> <p><strong>Because of Rafael Nadal:</strong> The GOAT player of Tennis in my opinion. I wish I could see him play live someday soon. His amazing athleticism, control over his emotions for not breaking his tennis racquet (like Djoko and others), and not being disrespectful or abusive to this opponent. For me, this is a standalone quality of an amazing professional sports player.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="bib"/><summary type="html"><![CDATA[There‚Äôs a yellow colored sticky note in my room that says, ‚ÄúSports build character and that‚Äôs why Tennis.‚Äù¬†Period.]]></summary></entry><entry><title type="html">The future of precision agriculture</title><link href="https://nitin-dominic.github.io/NR/NR/blog/2022/PA/" rel="alternate" type="text/html" title="The future of precision agriculture"/><published>2022-11-30T13:56:00+00:00</published><updated>2022-11-30T13:56:00+00:00</updated><id>https://nitin-dominic.github.io/NR/NR/blog/2022/PA</id><content type="html" xml:base="https://nitin-dominic.github.io/NR/NR/blog/2022/PA/"><![CDATA[<p>Recently, I was in a debate with a friend who seems to carry a negative impression on the use of technology to automate traditional farming practices. As I went deeper into the conversation, I realized that his arguments were specifically targeting the application of drones that promises to monitor agricultural farms by generating humongous amount of data. According to him, only humans are a best fit to be tasked with crop monitoring or site-specific application. Unlike machines or robots, humans can be relied for smart decision-making ability in the real-world scenario. As a counter reasoning, I presented my perspective on how present technologies are shaping the future of farming without disrupting the environment or rather in several cases they are smarter and faster in accomplishing tasks that demands automation due to its repetitive nature.</p> <p>Back in the early 1980s, the founding principle behind developing Precision Agriculture (PA) tech- nologies hinged on the fact that these technologies will be engineered to make farming practices more accurate and controlled when raising livestock or growing/monitoring crops. This principle in itself was so objective that engineers had to think and design technologies that could treat areas based on three significant factors: right location, right amount, and right time. To accomplish all the 3R‚Äôs, a computer decision making systems was supposed to be designed that would imitate humans in terms of making decisions on-the-go. It was in the late 1990s, a tech giant named John Deere ventured into this area and developed the first GPS-guided tractor that fetched coordinates of a location to guide tractors on farms. <strong>A 15,000 lbs machine was now able to make sense where it was exactly in the field</strong>. This technology was widely adopted by farmers that ultimately led to the rise of PA. Today, PA is booming with varied areas of research conducted across numerous universities and industries. What seemed impossible 50 years back is now made possible due to advancements made in deploying agricultural robots for planting, harvesting, weeding, spraying, etc. Thanks to advanced technologies like, Deep Learning, Computer Vision, and Motor Control approaches. A decade back on one would have imagined a flying saucer (drones at 32 ft altitude) that could be used to classify weeds amongst crop plants and subsequently spray herbicide as needed. With the help of complex designed sensors, crop yield and various soil properties can be estimated eliminating the need to perform traditional and arduous laboratory analysis. Application of cutting-edge technologies such as, cloud or edge-based computing is helping farmers know the status of their crops on-the-go. In short, this is exactly where the PA is headed towards in the near future.</p> <p>My personal take on the future of PA science and technology is this: I believe that PA is a con- glomeration of various other engineering and science discipline applied on an agronomic field. It uses the skillset from computer science for designing intelligent algorithms, complex sensors from electrical engineering domain, actuating a nozzle for spraying mechanism from mechatronics en- gineering, and the list could be added. My goal as a researcher who aspires to work closely with the PA domain in the future is to continuously think and innovate PA technology by learning skillset from various engineering discipline. <strong>As an agricultural engineer, my goal is not to eliminate humans with robots but to make their work easier by helping them collaborate with technology through automation</strong>. By doing this, I perceive that technologies like drones can favor in assisting farmers in making smart decision through big data and not disrupt the environment.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="bib"/><summary type="html"><![CDATA[Recently, I was in a debate with a friend who seems to carry a negative impression on the use of technology to automate traditional farming practices. As I went deeper into the conversation, I realized that his arguments were specifically targeting the application of drones that promises to monitor agricultural farms by generating humongous amount of data. According to him, only humans are a best fit to be tasked with crop monitoring or site-specific application. Unlike machines or robots, humans can be relied for smart decision-making ability in the real-world scenario. As a counter reasoning, I presented my perspective on how present technologies are shaping the future of farming without disrupting the environment or rather in several cases they are smarter and faster in accomplishing tasks that demands automation due to its repetitive nature.]]></summary></entry><entry><title type="html">Getting started with real-time image processing on Nvidia Jetson AGX Xavier</title><link href="https://nitin-dominic.github.io/NR/NR/blog/2021/Nvidia/" rel="alternate" type="text/html" title="Getting started with real-time image processing on Nvidia Jetson AGX Xavier"/><published>2021-06-30T13:56:00+00:00</published><updated>2021-06-30T13:56:00+00:00</updated><id>https://nitin-dominic.github.io/NR/NR/blog/2021/Nvidia</id><content type="html" xml:base="https://nitin-dominic.github.io/NR/NR/blog/2021/Nvidia/"><![CDATA[<p>This tutorial is for those who are new to using Jetson platform, specifically Nvidia AGX Xavier. Nvidia Jetson platforms are embedded systems (can fit on your palm!) known to perform edge computing (high-throughput computing) in real-time with dynamic environments. The application of this piece of hardware is emerging in the field agriculture where certain real-time applications such as, crop scouting (Yang et al., 2020), fruit detection (Mazzia et al., 2016), pruning applications (Mulhollem et al., 2020), etc. are on the rise as these systems are becoming more advanced in handling hefty amount of data for computer vision application. Therefore, I have jotted down some points (very straightforward to get you started) while I was getting started on this hardware to make it more easy and approachable for you without spending weeks if not months to get the system running up!</p> <p>A novice user can learn the basics of setting up the system (after completely finishing this blog) and further leverage (performing advanced image processing by developing their own scripts on python!) the power of this module in their area of research as well. Till now to this date, Nvidia has released several versions of these modules which differ in amount of RAM size with varying power consumptions. Performance of these systems vary as you spend more money ranging from the Jetson Nano to Xavier. 3 types of series available so far are:</p> <ul> <li>Jetson Nano series</li> <li>Jetson TX1/TX2</li> <li>Jetson Xavier series (NX/AGX) ‚Äî This blog covers this module</li> <li>Prerequisites before you start:</li> </ul> <p>You should know how to operate a Linux based system via command line An understanding of how cv2.capture (an OpenCV library) works Simple image processing basics (filtering, thresholding, and stuff!) Connecting it to a monitor I am writing this as a part of this blog because the first time when I used this module, I had no clue what to do. All I thought that the required tools and packages (such as, CUDA, cuDNN, TensorRT, OpenCV) to perform AI on this machine was already installed. You might feel the same when you connect this module with a monitor. An advice here would be: ‚Äúdo not try to use any converters to connect the module with the monitor. It simply supports HDMI to HDMI connection‚Äù. If you will try to use any converter, let‚Äôs say those blue colored display to HDMI convertors, the screen will not show any display. The first time you connect this display it opens up Ubuntu (Linux based OS, mine was 16.04).</p> <p>You need a remote PC which has Linux installed in it Yes, you need to connect this systems (via USB C to USB) to flash it. This flashing installs all the required packages needed. In short, sdk manager is a GUI which helps you install (via manual selection) all the packages needed to perform computer vision based tasks. Things you‚Äôll need to get started:</p> <p>Module (Nano, TX1/TX2/AGX/NX) Monitor to connect to the module A separate keyboard and a mouse An ethernet connection. Just in case if you have a Wifi chip, please install it on Xavier AGX to start wireless connection, A remote PC with Linux installed ‚Äî You‚Äôll need to have the same version of Ubuntu installed on your module as well as remote PC An extra set of keyboard and mouse Type C to USB to connect it from module to the remote PC Flashing it with a Jetpack using sdk manager First and foremost, make an account on Nvidia Developer (https://developer.nvidia.com/) and download sdk manager (*.deb) in your downloads folder in the remote PC. Once done, open the terminal and cd to downloads. Type this after that: Step 01‚Äî -&gt;</p> <p>sudo apt install ./sdkmanager_[version].[build#].deb (you can simply copy paste the file name you just downloaded) Once installed, type, sdkmanager. Login in with your ID and password (remember I asked you to make an account before!)</p> <ol> <li> <p>You‚Äôll see a GUI which looks like this (fig 1). You can select whichever Target Hardware you are planning to use for your work and make sure that your sdk manager already detects that there‚Äôs a Jetson machine attached with your remote PC. You can select the latest Linux Jetpack if you want. Continue to Step 02 ‚Äî -&gt;</p> </li> <li> <p>Step 02 is the meat of the whole installation. This is where all the required packages, libraries, tools, such as, CUDA, TensorRT, cuDNN, Computer Vision, and Developer Tools gets installed. Install it in your desired directory/location if you want.</p> </li> <li> <p>Installation takes about 1/2hr to 40min depending on your internet speed. If failed in the middle, retry it. If that doesn‚Äôt work either, try to start everything from the beginning.</p> </li> <li> <p>Once you see this screen (left), put your username and password, and Flash it! Once your Jetson module is flashed successfully, your Jetson will reboot asking you to enter your username and password again. Do it! Now after this step, you don‚Äôt need the remote PC.</p> </li> </ol> <p>Downloading GithHub Repositories and installing OpenCV Jetson Hacks (https://www.jetsonhacks.com/) does some cool stuff on this platform. Migrate to his repositories on his GitHub page (https://github.com/jetsonhacks) to explore more if you like.</p> <p>Since we are more inclined to use OpenCV for real-time applications, let us install OpenCV from the source so you can configure GStreamer (https://gstreamer.freedesktop.org/) support which supports the external (maybe USB webcam or Raspberry Pi cam) camera for real-time applications. Also, Xavier AGX Jetson board comes with different mode for you to select, such as, 10W, 15W, 30W, etc. These modes deliver different power to perform high throughput applications based on demanding tasks. For more, see this (https://www.jetsonhacks.com/2018/10/07/nvpmodel-nvidia-jetson-agx-xavier-developer-kit/). cd to the folder where you would like to install all the dependencies related to OpenCV. Make sure you have enough space in the drive! Follow these:</p> <p>$git clone https://github.com/jetsonhacks/buildOpenCVXavier (I am using this one since I am using AGX Xavier) Switch to the downloaded repository: cd buildOpenCVXavier Now you will see *.sh file inside this folder by typing ls. Once confirmed, type, $ ./buildAndPackageOpenCV.sh (building takes about 50 mins or 1hr) Now you will notice opencv folder installed in your drive, migrate to build folder inside the same opencv folder, $cd.. ‚Äî -&gt; $cd opencv ‚Äî -&gt; $cd build Once in, type, $ sudo apt-get install cmake-curses-gui. This opens up a list of packages/dependencies for you to manually configure as you like. I don‚Äôt think this is a necessary step but as usual you can always explore! Deploying the real-time processing using a webcam I am adding some cool pics I clicked when I finished everything successfully.</p> <p>Figure. Canny Edge and 4-split screens with live camera feed, canny edge, Gaussian Filter Now its all straightforward. Migrate to Examples folder inside the cloned repository, buildOpenCVXavier. Once in, you will see there‚Äôs already one program written for (as cloned from the repository!) you, called as, cannyDetection.py. This is a simple edge detection algorithm. This is important. If you are working on Jetson AGX Xavier, you will need to make a very minute change in the code to make it stream via USB Webcam. If you plan to use, TX2/Nano, I don‚Äôt think so that change is necessary. Therefore open the code, and look for this line, def open_camera_device(device_number):</p> <p>return cv2.VideoCapture(device_number),</p> <p>Change device_number to 0 inside the curve bracket in the second line. This should let the GStreamer stream live video feed through your webcam. Once done, save and type this in the terminal,</p> <ol> <li>$ ./cannyDetection.py ‚Äî video_device 1. And you will see something like this (Figure above with you sitting infront!)</li> </ol> <p>You can play around with this stuff, create your own image processing algorithms and deploy. Have fun!</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="bib"/><summary type="html"><![CDATA[This tutorial is for those who are new to using Jetson platform, specifically Nvidia AGX Xavier. Nvidia Jetson platforms are embedded systems (can fit on your palm!) known to perform edge computing (high-throughput computing) in real-time with dynamic environments. The application of this piece of hardware is emerging in the field agriculture where certain real-time applications such as, crop scouting (Yang et al., 2020), fruit detection (Mazzia et al., 2016), pruning applications (Mulhollem et al., 2020), etc. are on the rise as these systems are becoming more advanced in handling hefty amount of data for computer vision application. Therefore, I have jotted down some points (very straightforward to get you started) while I was getting started on this hardware to make it more easy and approachable for you without spending weeks if not months to get the system running up!]]></summary></entry><entry><title type="html">The quest to become Homo Deus</title><link href="https://nitin-dominic.github.io/NR/NR/blog/2020/HomoDeus/" rel="alternate" type="text/html" title="The quest to become Homo Deus"/><published>2020-07-30T13:56:00+00:00</published><updated>2020-07-30T13:56:00+00:00</updated><id>https://nitin-dominic.github.io/NR/NR/blog/2020/HomoDeus</id><content type="html" xml:base="https://nitin-dominic.github.io/NR/NR/blog/2020/HomoDeus/"><![CDATA[<p><code class="language-plaintext highlighter-rouge">‚ÄúThe genie is out of the bottle. We need to move forward on artificial intelligence development but we also need to be mindful of its very real dangers. I fear that AI may replace humans altogether. If people design computer viruses, someone will design AI that replicates itself. This will be a new form of life that will outperform humans‚Äù. ~ Prof. Stephen William Hawking (Interview with Wired, November 2017)</code></p> <h3 id="what-exactly-is-the-root">What exactly is the root?</h3> <p><code class="language-plaintext highlighter-rouge">‚ÄúWe humans are blobs of organized mud, which through the impersonal workings of nature‚Äôs pattern have developed the capacity to contemplate and cherish and engage with the intimidating complexity of the world around us. The meaning we find in life is not transcendent‚Äù. ~ Sean M. Carroll (Physicist, specializing in Quantum Mechanics, gravity and cosmology, CALTECH).</code></p> <p>I was watching a 2-hour lecture given by Prof. John Lennox (Emeritus Fellow in Mathematics and Philosophy of Science, Oxford University) on ‚ÄúShould we fear Artificial Intelligence‚Äù? He described the two quintessential agendas of 21st the century which caught my attention, they are, a serious bid for immortality (where death has now become more of a technical problem) and intensification of the pursuit of happiness (in short pleasure!). He further quotes, ‚ÄúIt will be necessary to change our biochemistry and re-engineer our bodies and minds so that we shall need to re-engineer Homo Sapiens so that it can enjoy everlasting pleasure. Having raised humanity above the beastly level of survival struggles, we will now aim to upgrade into gods and turn Homo Sapiens into Homo Deus‚Äù (From the book, Homo Deus: A brief History of tomorrow by Yuval Noah Harari).</p> <h3 id="my-thought">My Thought</h3> <p>This has been a very common problem in the past. A problem that finds its grounding when humans are curiously wanting to place themselves at the seat of God. In simpler terms defining good and evil by themselves (Refer Genesis 3, you shall be as God, knowing good and evil, paraphrased). This may seem all new because it has a touch of scientism and technicality in it. The craving to know everything and achieve everything is what leads to this quest of living a life as Homo Deus.</p> <h3 id="ai-and-humans-are-different">AI and humans are different</h3> <p><code class="language-plaintext highlighter-rouge">‚ÄúIt seems to me that a lot of needless debate could be avoided if AI researchers would admit that there are fundamental differences between machine intelligence and human intelligence. Differences that cannot be overcome by any amount of research. In other words, the ‚Äúartificial‚Äù in artificial intelligence is real‚Äù. ~ Prof. Joseph Macrae Mellichamp, Emeritus Fellow of Management Sciences, University of Alabama.</code></p> <p>The assertion that the AI or ASI (Artificial superintelligence) and humans have vast differences are very true. I remember watching a movie in my 11th grade called ‚ÄúRobot‚Äù. I was seriously led into thinking that a human could possibly design a machine which could interact with the world around them with such freedom. Remember this is in 2011. Now exactly after 5 years a human robot named Sophia was activated on February 14, 2016, at South West Festival (SXSW), Texas. Possibly you can think of a future 5 years from now on. But I want to draw the reader‚Äôs attention on something else. There‚Äôs a key element that went missing in the life of that robot which is found in abundance in humans. Emotions and feelings. The plot of the movie was very hypothetical about the way the emotions were fed to the robot. The movie ended with a very drastic catastrophe caused by the robot. Possibly, it became more powerful than humans. Let‚Äôs consider several dissimilarities or uncommon characteristics between humans and robots.</p> <h3 id="emotions-and-feelings--my-thought">Emotions and Feelings | My Thought</h3> <p>We can have zero confidence that the dominant intelligence a few centuries hence will have any emotional resonance with us ‚Äì even though they may have an algorithmic understanding of the way we behaved‚Äù. ~ Prof. Martin Rees (British cosmologist and astrophysicist, University of Cambridge). May I add a little elaboration about the above-stated assertion. Though the robots may be trained to have an understanding of the way humans behave, it can never reach the way humans are made to develop feelings for other humans. For example, if I as a human is in love with someone, the emotions and feelings for that very person comes from within. No one has to program it for me in order to love the other person. But you see that‚Äôs not the case with the robots. For robots to know and love other robots or humans they have to be programmed the way humans work. And so, the robots cannot do things on their own provided it is trained by a more intellectual mind, humans.</p> <h3 id="conscious-thinking-and-intelligence--my-thought">Conscious thinking and intelligence | My Thought</h3> <p><code class="language-plaintext highlighter-rouge">‚ÄúBut here is what we are not born with: information, data, rules, software, knowledge, lexicons, representations, algorithms, programs, models, memories, images, processors, subroutines, encoders, decoders, symbols or buffers ‚Äì design elements that allow digital computers to behave somewhat intelligently. Not only are we not born with such things, we also don‚Äôt develop them ‚Äì ever. Computers, quite literally process information ‚Äì numbers, letters, words, formulas, images. Humans, on the other hand, do not ‚Äì never did, never will. It is one thing to say that brain functions like a computer. It is an entirely different thing to say that it is a computer‚Äù. ~ Roger Epstein (former editor of Psychology Today, ‚ÄúThe empty brain‚Äù).</code></p> <p>Roger Epstein has pretty much summed up the understanding between a human and a computer. Think like this. If I‚Äôm witnessing a predicament that I never ever witnessed before (completely blank, not even knowing what it means), would I be able to take the right decision or any decision? Now the answer for this question can be, yes I can take it, doesn‚Äôt matter if it‚Äôs right or wrong. But, that‚Äôs not the case with a robot. In order to take certain decision, a robot has to be programmed, in order to take certain decision or do a certain task. He cannot perform a task as we humans perform. This is what we call as ‚Äúcognitive consciousness‚Äù and ‚Äúawareness‚Äù. Robots can be intelligent (more than humans) but they are not conscious. AI separates intelligence and consciousness but remember that God links them.</p> <h3 id="final-thought">Final Thought</h3> <p><code class="language-plaintext highlighter-rouge">‚ÄúWe have paid some high prices for the technological conquest of nature, but none so high as the intellectual and spiritual costs of seeing nature as mere material for our manipulation, exploitation and transformation. With the powers of biological engineering gathering, there will be splendid new opportunities for similar degradation of our view of man. If we come to see ourselves as meat, then meat we shall become.‚Äù ~ Leon Kass (American Physician, Harvard University)</code></p> <p>I don‚Äôt have the liberty to keep on writing in lengths in this blog. But as usual final thoughts do count as they conclude the whole story about the blog. Super creation already exists. Then, who are they? The answer is simple. It‚Äôs us. Humans. Humans can and will create robots, but one thing that they will never be able to fulfil or feed in those robots, and that is their understanding of the world around them. This understanding comes from within. They can never provide an identity to the robots. They can never give different fingerprints to different robots. They can never give a different DNA to a different robot. They can never give free will to the robots. A free will to make choice between good and evil. These things sum up to one thing and that is they can never give meaning (to understand what life is all about) to a robot. Well, only God wins in the end. A God who was and is big enough to create humans in His image (Read Genesis 1:26-28). That‚Äôs exactly where life of conscious thinking and intelligence began. That‚Äôs exactly where search for a new Homo Deus is worthless.</p> <p><code class="language-plaintext highlighter-rouge">‚ÄúSuper creation already exists. God is primary, universe is derivative, Creation by speech act, ‚Äúand God said‚Äù. God speaking from an open system and not a closed natural framework‚Äù. ~ John Lennox (Emeritus Fellow in Mathematics and Philosophy of Science, Oxford University).</code></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="bib"/><summary type="html"><![CDATA[‚ÄúThe genie is out of the bottle. We need to move forward on artificial intelligence development but we also need to be mindful of its very real dangers. I fear that AI may replace humans altogether. If people design computer viruses, someone will design AI that replicates itself. This will be a new form of life that will outperform humans‚Äù. ~ Prof. Stephen William Hawking (Interview with Wired, November 2017)]]></summary></entry></feed>