<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://nitin-dominic.github.io/NR/NR/feed.xml" rel="self" type="application/atom+xml"/><link href="https://nitin-dominic.github.io/NR/NR/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-12-08T19:22:34+00:00</updated><id>https://nitin-dominic.github.io/NR/NR/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">The script that helped me win 3MT thesis competition</title><link href="https://nitin-dominic.github.io/NR/NR/blog/2023/3MT-Thesis/" rel="alternate" type="text/html" title="The script that helped me win 3MT thesis competition"/><published>2023-02-16T13:56:00+00:00</published><updated>2023-02-16T13:56:00+00:00</updated><id>https://nitin-dominic.github.io/NR/NR/blog/2023/3MT-Thesis</id><content type="html" xml:base="https://nitin-dominic.github.io/NR/NR/blog/2023/3MT-Thesis/"><![CDATA[<p>After much considerations, I finally plan on writing a small post about the script (summary of my research) that helped me win NDSU‚Äôs annual 3MT Thesis competition. This was my first time (since the time I joined NDSU in the fall 2019) participating in the competition with 2-levels of rounds, preliminary and championship. Below is the script that I wrote and memorized it really hard while trying to speak atleat 5-times a day so that I don‚Äôt forget it when facing a large audience. My presentation was titled, <strong>Drone spots illegal weed plants: The future of agriculture takes flight</strong>. I have provided some strike-throughs (<del>this</del>, with changes made in the brackets) throughout the script that basically portrays how I navigated my script for the final talk after gathering advise from a mentor. Also, the bold fonts (<strong>This one</strong>) mean that an expression or an emphasis was required. üë®‚Äçüî¨</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/NR/assets/img/3MT_Nitin-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/NR/assets/img/3MT_Nitin-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/NR/assets/img/3MT_Nitin-1400.webp"/> <img src="/NR/assets/img/3MT_Nitin.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Dr. Colleen Fitzgerald, Vice President for Research and Creative Activity at North Dakota State University, handing over the prize money to me right after the competition. </div> <p><em>The script starts below‚Ä¶</em></p> <p>Have you ever <strong>wondered</strong> if an uncrewed aircraft system (UAS) or drones could be used to monitor agricultural farms? Well, I carried the <strong>same curiosity</strong> almost 3-and-a-half years back and now my research focuses on the application of drones in agriculture.</p> <p><em>My name is Nitin Rai and I am a Ph.D. candidate in the department of Agricultural and Biosystems Engineering.</em></p> <p>According to a survey, the world population is about to reach <strong>9 billion mark</strong> by the end of 2030. With this population growth rate, growers and farmers <strong>are expected to</strong> increase the use of technolgies that are reliable, fast, and sustainable. One such technologies are drones in agriculture that can be used to monitor weed growth. Now as we all know that weeds are unwanted plants that compete with crop plants for sunlight, water, and nutrients <del>thereby affecting its yield</del> (and what not). For example, a weed by the name of kochia was found to decrease corn production by over 90%. <del>These weeds could be identified by in-field machines and eradicated on-the-go.</del> (<strong>But there‚Äôs a hope!</strong>). Do you know that these weeds could be eliminated by such advanced technologies in real-time?</p> <p>Well allow me to do some time-travelling here <del>by taking you half a century back</del>. In the late 1970s, farmers had to <del>literally</del> walk miles in order to manually <del>apply herbicide solutions</del> (eliminate weeds). <del>Fast forwarded to</del> Another 20 years <del>and engineers developed conventional sprayers</del> (and we have big conventional sprayers). Fast-forwarded to another 20 years and we have drone technology. <strong>So, as a 21st century agricultural engineer, what role do I play in making advancements and contributions to such technologies?</strong> Well, the answer to this is: I teach drones to identify weeds <del>and that‚Äôs where I believe the future of agriculture is headed.</del> (amongst crop plants).</p> <p>So, questioning the gist of my research; how do I teach drones? <del>to spot weeds amongst crop plants?</del> Imagine teaching a 4 year old kid about how an apple looks like? Well you may <del>High chances are that you‚Äôre going to</del> start by describing its textures, such as, an apple is red color, it has a crisp texture, or is roundish in shape. <del>You may also show an image of an apple with different backgrounds and the next time the kid is with you in Walmart, he‚Äôll certainly point out those red apples for a bite!</del> <strong>A computer is just like that 4 year old kid</strong>. In our research we collect <strong>thousands of images of weeds</strong> and <del>train</del> feed it <del>using an approach</del> in an algorithm called as machine learning algorithm. What these machine learning algorithms are capable of <del>doing</del> achieving is extracting <del>very specific features pertaining to</del> relevant features from a specific class of weed specie. <del>We then take these modeled algorithms and deploy them in multiple unseen locations.</del> Once we have these features modeled, we then take these models to multiple field locations to identify those weed species. <del>Once the algorithm identifies the weeds, we compare it with the ground truth data which are labeled by human experts.</del></p> <p>As far as the results <del>from our research is concerned</del> are concerned, our <del>trained</del> algorithm/model has been able to achieve an <strong>overall accuracy of over 85% in identifying 5 different species of weeds across 2 location in the state of ND</strong>. <del>With this accuracy</del> Moreover, our algorithm/model can be integrated with the technologies that are used by growers and farmers <del>would be able to integrate this technology with their spraying</del> to identify and spray weeds in their field as well. technology. <del>Moreover, our algorithm could be trained on small computers in less hours.</del></p> <p>So, does the future of agriculture takes flight <del>by monitoring your agricultural field</del>? <strong>Yes, of course it does!</strong> Therefore, the next time when you are about to eat a delicious meal, <strong>think about these technologies and farmers effort</strong> in quenching your starving tummy.</p> <p>Thank you!!! üòÉ</p> <p>P.S: If you‚Äôre interested in watching a recorded presentation, please click <a href="https://youtu.be/j3DfPBQiBS4">here</a>ü•á.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[After much considerations, I finally plan on writing a small post about the script (summary of my research) that helped me win NDSU‚Äôs annual 3MT Thesis competition. This was my first time (since the time I joined NDSU in the fall 2019) participating in the competition with 2-levels of rounds, preliminary and championship. Below is the script that I wrote and memorized it really hard while trying to speak atleat 5-times a day so that I don‚Äôt forget it when facing a large audience. My presentation was titled, Drone spots illegal weed plants: The future of agriculture takes flight. I have provided some strike-throughs (this, with changes made in the brackets) throughout the script that basically portrays how I navigated my script for the final talk after gathering advise from a mentor. Also, the bold fonts (This one) mean that an expression or an emphasis was required. üë®‚Äçüî¨]]></summary></entry><entry><title type="html">Tennis - Why do I love this sport?</title><link href="https://nitin-dominic.github.io/NR/NR/blog/2022/Tennis/" rel="alternate" type="text/html" title="Tennis - Why do I love this sport?"/><published>2022-12-31T13:56:00+00:00</published><updated>2022-12-31T13:56:00+00:00</updated><id>https://nitin-dominic.github.io/NR/NR/blog/2022/Tennis</id><content type="html" xml:base="https://nitin-dominic.github.io/NR/NR/blog/2022/Tennis/"><![CDATA[<blockquote> <p>There‚Äôs a yellow colored sticky note in my room that says, ‚ÄúSports build character and that‚Äôs why Tennis.‚Äù¬†Period.</p> </blockquote> <p>When I was 12 or 13, I vividly remember asking my dad if he could look for some tennis courts in the city (I did not own an Android phone back then!) and talk with the authorities to get me registered for playing tennis. The city that I was raised in India had only limited number of tennis courts owned by some big Clubs that charged high annual membership fees to allow its members to play tennis. With tennis in my heart, I grew up watching Federer and Nadal play and often used to think that hitting balls the way they did was easy. All you have to do is to be very energetic and active while you‚Äôre on the court. Alas! I WAS WRONG!!!</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/NR/assets/img/tennis-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/NR/assets/img/tennis-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/NR/assets/img/tennis-1400.webp"/> <img src="/NR/assets/img/tennis.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Me trying a backhand while playing on one of the University Village Courts near NDSU-Main Campus. </div> <p>Years passed by and I never got a chance to play tennis until 2017. I got into IIT KGP to pursue my master‚Äôs study and it was for the first time when I saw 5‚Äì6 open tennis courts on-campus. My love for tennis was rejuvenated. But the biggest hurdle was to buy a good racquet and a set of balls to play with (which indeed was expensive!). I was a bit hesitant to ask my dad to sponsor a significant amount just to buy a racquet and set of balls (although I knew he would definitely pay for it!). However, I was able to talk with someone and managed to procure a Wilson racquet. Since then, I have been playing tennis and enjoying the sport more each day (but I am not consistent at all). Over time I have thought that I played many sports including, Volleyball, Cricket (I don‚Äôt like Cricket at all for some reason!), Squash, Badminton, Football (Soccer and not the American Football), Table-Tennis (Ping-pong), and Tennis (of-course). But I love tennis the most and is special to me due to following reasons:</p> <p><strong>Tennis demands mental strength:</strong> I won‚Äôt be bias at all if I say that tennis demands mental strength. That does not mean that the other sports does not. But playing tennis as singles is really exhausting and that too for 5-6 sets straight (think about the professionals!). It demands strong characters of an athelete. Tennis also involves strategic thinking ability to attack your opponent‚Äô weakness + judging the balls. One has to be actively thinking and ‚ÄúBE PRESENT MENTALLY‚Äù on the court while playing this sport.</p> <p><strong>Tennis demands physical strength that leads to hitting strategic shots:</strong> This part is directly linked to the mental strength. If one can think to attack the opponent, one has to act too. In spite of not just simply running on the court, one should have charasmatic reflexes too in order to hit the ball just in the direction they might have thought. It requires a coordination between what one thinks and how one acts. ¬†</p> <p><strong>Fighting for each point</strong>: In a way, tennis has taught me so much about life. No matter how challenging life can get on an every day basis (for points when playing), one always get an opportunity to fight back sometime!¬†One of the major reason when I am on the court, I always have this inner feeling to fight and play the best as per my strength both mentally and physically.</p> <p><strong>Because of Rafael Nadal:</strong> The GOAT player of Tennis in my opinion. I wish I could see him play live someday soon. His amazing athleticism, control over his emotions for not breaking his tennis racquet (like Djoko and others), and not being disrespectful or abusive to this opponent. For me, this is a standalone quality of an amazing professional sports player.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[There‚Äôs a yellow colored sticky note in my room that says, ‚ÄúSports build character and that‚Äôs why Tennis.‚Äù¬†Period.]]></summary></entry><entry><title type="html">The future of precision agriculture</title><link href="https://nitin-dominic.github.io/NR/NR/blog/2022/PA/" rel="alternate" type="text/html" title="The future of precision agriculture"/><published>2022-11-30T13:56:00+00:00</published><updated>2022-11-30T13:56:00+00:00</updated><id>https://nitin-dominic.github.io/NR/NR/blog/2022/PA</id><content type="html" xml:base="https://nitin-dominic.github.io/NR/NR/blog/2022/PA/"><![CDATA[<p>Recently, I was in a debate with a friend who seems to carry a negative impression on the use of technology to automate traditional farming practices. As I went deeper into the conversation, I realized that his arguments were specifically targeting the application of drones that promises to monitor agricultural farms by generating humongous amount of data. According to him, only humans are a best fit to be tasked with crop monitoring or site-specific application. Unlike machines or robots, humans can be relied for smart decision-making ability in the real-world scenario. As a counter reasoning, I presented my perspective on how present technologies are shaping the future of farming without disrupting the environment or rather in several cases they are smarter and faster in accomplishing tasks that demands automation due to its repetitive nature.</p> <p>Back in the early 1980s, the founding principle behind developing Precision Agriculture (PA) tech- nologies hinged on the fact that these technologies will be engineered to make farming practices more accurate and controlled when raising livestock or growing/monitoring crops. This principle in itself was so objective that engineers had to think and design technologies that could treat areas based on three significant factors: right location, right amount, and right time. To accomplish all the 3R‚Äôs, a computer decision making systems was supposed to be designed that would imitate humans in terms of making decisions on-the-go. It was in the late 1990s, a tech giant named John Deere ventured into this area and developed the first GPS-guided tractor that fetched coordinates of a location to guide tractors on farms. <strong>A 15,000 lbs machine was now able to make sense where it was exactly in the field</strong>. This technology was widely adopted by farmers that ultimately led to the rise of PA. Today, PA is booming with varied areas of research conducted across numerous universities and industries. What seemed impossible 50 years back is now made possible due to advancements made in deploying agricultural robots for planting, harvesting, weeding, spraying, etc. Thanks to advanced technologies like, Deep Learning, Computer Vision, and Motor Control approaches. A decade back on one would have imagined a flying saucer (drones at 32 ft altitude) that could be used to classify weeds amongst crop plants and subsequently spray herbicide as needed. With the help of complex designed sensors, crop yield and various soil properties can be estimated eliminating the need to perform traditional and arduous laboratory analysis. Application of cutting-edge technologies such as, cloud or edge-based computing is helping farmers know the status of their crops on-the-go. In short, this is exactly where the PA is headed towards in the near future.</p> <p>My personal take on the future of PA science and technology is this: I believe that PA is a con- glomeration of various other engineering and science discipline applied on an agronomic field. It uses the skillset from computer science for designing intelligent algorithms, complex sensors from electrical engineering domain, actuating a nozzle for spraying mechanism from mechatronics en- gineering, and the list could be added. My goal as a researcher who aspires to work closely with the PA domain in the future is to continuously think and innovate PA technology by learning skillset from various engineering discipline. <strong>As an agricultural engineer, my goal is not to eliminate humans with robots but to make their work easier by helping them collaborate with technology through automation</strong>. By doing this, I perceive that technologies like drones can favor in assisting farmers in making smart decision through big data and not disrupt the environment.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Recently, I was in a debate with a friend who seems to carry a negative impression on the use of technology to automate traditional farming practices. As I went deeper into the conversation, I realized that his arguments were specifically targeting the application of drones that promises to monitor agricultural farms by generating humongous amount of data. According to him, only humans are a best fit to be tasked with crop monitoring or site-specific application. Unlike machines or robots, humans can be relied for smart decision-making ability in the real-world scenario. As a counter reasoning, I presented my perspective on how present technologies are shaping the future of farming without disrupting the environment or rather in several cases they are smarter and faster in accomplishing tasks that demands automation due to its repetitive nature.]]></summary></entry><entry><title type="html">Getting started with real-time image processing on Nvidia Jetson AGX Xavier</title><link href="https://nitin-dominic.github.io/NR/NR/blog/2021/Nvidia/" rel="alternate" type="text/html" title="Getting started with real-time image processing on Nvidia Jetson AGX Xavier"/><published>2021-06-30T13:56:00+00:00</published><updated>2021-06-30T13:56:00+00:00</updated><id>https://nitin-dominic.github.io/NR/NR/blog/2021/Nvidia</id><content type="html" xml:base="https://nitin-dominic.github.io/NR/NR/blog/2021/Nvidia/"><![CDATA[<p>This tutorial is for those who are new to using Jetson platform, specifically Nvidia AGX Xavier. Nvidia Jetson platforms are embedded systems (can fit on your palm!) known to perform edge computing (high-throughput computing) in real-time with dynamic environments. The application of this piece of hardware is emerging in the field agriculture where certain real-time applications such as, crop scouting (Yang et al., 2020), fruit detection (Mazzia et al., 2016), pruning applications (Mulhollem et al., 2020), etc. are on the rise as these systems are becoming more advanced in handling hefty amount of data for computer vision application. Therefore, I have jotted down some points (very straightforward to get you started) while I was getting started on this hardware to make it more easy and approachable for you without spending weeks if not months to get the system running up!</p> <p>A novice user can learn the basics of setting up the system (after completely finishing this blog) and further leverage (performing advanced image processing by developing their own scripts on python!) the power of this module in their area of research as well. Till now to this date, Nvidia has released several versions of these modules which differ in amount of RAM size with varying power consumptions. Performance of these systems vary as you spend more money ranging from the Jetson Nano to Xavier. 3 types of series available so far are:</p> <ul> <li>Jetson Nano series</li> <li>Jetson TX1/TX2</li> <li>Jetson Xavier series (NX/AGX) ‚Äî This blog is about this module</li> </ul> <h3 id="prerequisites-before-you-start">Prerequisites before you start:</h3> <ul> <li>You should have a basic understanding of Linux OS</li> <li>An understanding of how cv2.capture (an OpenCV library) works</li> <li>Basic wokring of image processing concepts such as filtering or thresholding</li> <li>Connecting it to a monitor (use an HDMI cable)</li> </ul> <p>I am writing this as a part of this blog because the first time when I used this module, I had no clue what to do. All I thought that the required tools and packages (such as, CUDA, cuDNN, TensorRT, OpenCV) to perform AI on this machine was already installed. You might feel the same when you connect this module with a monitor. An advice here would be: ‚Äúdo not try to use any converters to connect the module with the monitor. It simply supports HDMI to HDMI connection‚Äù. If you will try to use any converter, let‚Äôs say those blue colored display to HDMI convertors, the screen will not show any display. The first time you connect this display it opens up Ubuntu (Linux based OS, mine was 16.04). You will also need a remote PC which has Linux installed in it. Yes, you need to connect this systems (via USB C to USB) to flash it. This flashing installs all the required packages needed. In short, <code class="language-plaintext highlighter-rouge">sdk manager</code> is a GUI which helps you install (via manual selection) all the packages needed to perform computer vision based tasks. Things you‚Äôll need to get started:</p> <ul> <li>Module (Nano, TX1/TX2/AGX/NX)</li> <li>Monitor to connect to the module</li> <li>A separate keyboard and a mouse</li> <li>An ethernet connection. Just in case if you have a Wifi chip, please install it on Xavier AGX to start wireless connection,</li> <li>A remote PC with Linux installed ‚Äî You‚Äôll need to have the same version of Ubuntu installed on your module as well as remote PC</li> <li>An extra set of keyboard and mouse</li> <li>Type C to USB to connect it from module to the remote PC</li> <li>Flashing it with a Jetpack using sdk manager</li> <li>First and foremost, make an account on Nvidia Developer (https://developer.nvidia.com/) and download sdk manager (*.deb) in your downloads folder in the remote PC. Once done, open the terminal and cd to downloads. Type this after that: Step 01‚Äî -&gt;</li> </ul> <p><code class="language-plaintext highlighter-rouge">sudo apt install ./sdkmanager_[version].[build#].deb</code> (you can simply copy paste the file name you just downloaded)</p> <p>Once installed, type, sdkmanager. Login in with your ID and password (remember I asked you to make an account before!)</p> <ol> <li>You‚Äôll see a GUI which looks like this (fig 1). You can select whichever Target Hardware you are planning to use for your work and make sure that your sdk manager already detects that there‚Äôs a Jetson machine attached with your remote PC. You can select the latest Linux Jetpack if you want. Continue to Step 02 ‚Äî -&gt;</li> <li>Step 02 is the meat of the whole installation. This is where all the required packages, libraries, tools, such as, CUDA, TensorRT, cuDNN, Computer Vision, and Developer Tools gets installed. Install it in your desired directory/location if you want.</li> <li>Installation takes about 1/2hr to 40min depending on your internet speed. If failed in the middle, retry it. If that doesn‚Äôt work either, try to start everything from the beginning.</li> <li>Once you see this screen (left), put your username and password, and Flash it! Once your Jetson module is flashed successfully, your Jetson will reboot asking you to enter your username and password again. Do it! Now after this step, you don‚Äôt need the remote PC.</li> </ol> <p>Downloading GithHub Repositories and installing OpenCV Jetson Hacks (https://www.jetsonhacks.com/) does some cool stuff on this platform. Migrate to his repositories on his <a href="https://github.com/jetsonhacks">GitHub page</a> to explore more if you like.</p> <p>Since we are more inclined to use OpenCV for real-time applications, let us install OpenCV from the source so you can configure GStreamer (https://gstreamer.freedesktop.org/) support which supports the external (maybe USB webcam or Raspberry Pi cam) camera for real-time applications. Also, Xavier AGX Jetson board comes with different mode for you to select, such as, 10W, 15W, 30W, etc. These modes deliver different power to perform high throughput applications based on demanding tasks. For more, see this (https://www.jetsonhacks.com/2018/10/07/nvpmodel-nvidia-jetson-agx-xavier-developer-kit/). cd to the folder where you would like to install all the dependencies related to OpenCV. Make sure you have enough space in the drive! Follow these:</p> <p>$git clone https://github.com/jetsonhacks/buildOpenCVXavier (I am using this one since I am using AGX Xavier) Switch to the downloaded repository:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl"> Using Linux terminal
</span><span class="sb">cd buildOpenCVXavier```
</span></code></pre></div></div> <p>Now you will see *.sh file inside this folder by typing ls. Once confirmed, type, $ ./buildAndPackageOpenCV.sh (building takes about 50 mins or 1hr) Now you will notice opencv folder installed in your drive, migrate to build folder inside the same opencv folder, $cd.. ‚Äî -&gt; $cd opencv ‚Äî -&gt; $cd build Once in, type, $ sudo apt-get install cmake-curses-gui. This opens up a list of packages/dependencies for you to manually configure as you like. I don‚Äôt think this is a necessary step but as usual you can always explore! Deploying the real-time processing using a webcam. Now its all straightforward. Migrate to Examples folder inside the cloned repository, buildOpenCVXavier. Once in, you will see there‚Äôs already one program written for (as cloned from the repository!) you, called as, cannyDetection.py. This is a simple edge detection algorithm. This is important. If you are working on Jetson AGX Xavier, you will need to make a very minute change in the code to make it stream via USB Webcam. If you plan to use, TX2/Nano, I don‚Äôt think so that change is necessary. Therefore open the code, and look for these line,</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl"> Python file
</span><span class="sb">def open_camera_device(device_number):
return cv2.VideoCapture(device_number),
</span></code></pre></div></div> <p>Change device_number to 0 inside the curve bracket in the second line. This should let the GStreamer stream live video feed through your webcam. Once done, save and type this in the terminal,</p> <ol> <li><code class="language-plaintext highlighter-rouge">$ ./cannyDetection.py ‚Äî video_device 1.</code> And you will see something like this (see below)</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/NR/assets/img/ip1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/NR/assets/img/ip1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/NR/assets/img/ip1-1400.webp"/> <img src="/NR/assets/img/ip1.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/NR/assets/img/ip2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/NR/assets/img/ip2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/NR/assets/img/ip2-1400.webp"/> <img src="/NR/assets/img/ip2.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple yet effective way to deploy real-time image processing algorithms on Nvidia Jetson module. </div> <p>You can play around with this stuff, create your own image processing algorithms and deploy. Have fun!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[This tutorial is for those who are new to using Jetson platform, specifically Nvidia AGX Xavier. Nvidia Jetson platforms are embedded systems (can fit on your palm!) known to perform edge computing (high-throughput computing) in real-time with dynamic environments. The application of this piece of hardware is emerging in the field agriculture where certain real-time applications such as, crop scouting (Yang et al., 2020), fruit detection (Mazzia et al., 2016), pruning applications (Mulhollem et al., 2020), etc. are on the rise as these systems are becoming more advanced in handling hefty amount of data for computer vision application. Therefore, I have jotted down some points (very straightforward to get you started) while I was getting started on this hardware to make it more easy and approachable for you without spending weeks if not months to get the system running up!]]></summary></entry><entry><title type="html">Object detection in UAS-acquired imagery using ArcGIS Pro and deep learning</title><link href="https://nitin-dominic.github.io/NR/NR/blog/2020/ArcGISPro/" rel="alternate" type="text/html" title="Object detection in UAS-acquired imagery using ArcGIS Pro and deep learning"/><published>2020-08-20T13:56:00+00:00</published><updated>2020-08-20T13:56:00+00:00</updated><id>https://nitin-dominic.github.io/NR/NR/blog/2020/ArcGISPro</id><content type="html" xml:base="https://nitin-dominic.github.io/NR/NR/blog/2020/ArcGISPro/"><![CDATA[<p>This write up/tutorial is for those who are currently involved with working on ArcGIS Pro and want to learn a bit about Deep Learning too. Although, Deep Learning can be executed and worked independently using Python and other common platforms, I‚Äôll explain how can we integrate Deep Learning in ArcGIS Pro. If you already know how to do that, you may even choose to skip reading the write up. ArcGIS Pro has recently released 2.6 version which involves installing different newer version of Deep Learning packages within ArcGIS Pro. What needs to be noted down here is that there are several specific package versions of Deep Learning tools for ArcGIS Pro 2.5v and 2.6v. Pay attention while installing those packages because even if you miss out one package version you will end up in a lot of errors which is probably not desired to make you feel more frustrated. I have jotted down all the specific version for ArcGIS Pro 2.5v and 2.6v. Not only this but also, I have included few codes which you can write in python (just to automatize and save some time without much clicks!). Although you will find all these instructions on ESRI website (Deep Learning in ArcGIS Pro), you may have to browse through a lot of web pages back and forth to gather information from all sides. I have included all the details right here needed to integrate Deep Learning in ArcGIS Pro.</p> <div class="notice--danger"> <p>Before starting, please be mindful that the deep learning packages installed in this blog are outdated. Please check the latest versions within the software and install accordingly!</p> </div> <h3 id="1-installing-deep-learning-tools-in-arcgis-pro">1. Installing Deep Learning Tools in ArcGIS Pro</h3> <p><strong>a. ArcGIS Pro (2.5v)</strong></p> <ol> <li> <p>To begin, download Anaconda with a Python 3.6v (as I did in my case)</p> </li> <li> <p>Open Python Command Prompt and write these lines <em>(italicized)</em></p> </li> </ol> <p>In the place of deeplearning_arcgispro you can put any name you want. This creates an environment and clones everything from arcgispro-py3 which is already present in ArcGIS Pro folder when you initially installed it. This will also take few minutes to clone. After you have successfully cloned arcgispro-py3, you can see it by following this path, C:\Users&lt;username&gt;\AppData\Local\ESRI\conda\envs\deeplearning. Also please install all these in a newly created environment (folder).</p> <p><code class="language-plaintext highlighter-rouge">conda create ‚Äìname deeplearning_arcgispro ‚Äìclone arcgispro-py3</code></p> <p>now activate the created deeplearning_arcgispro envs</p> <p><code class="language-plaintext highlighter-rouge">activate deeplearning_arcgispro</code></p> <p>begin installing the packages (be specific with the versions here). Also, for those who doesn‚Äôt own a PC with Nvidia GPU and wish to run TensorFlow on a CPU instead of a GPU, you can add a package called <code class="language-plaintext highlighter-rouge">‚Äútensorflow-mkl‚Äù</code> from the Python Package Manager in ArcGIS Pro itself.</p> <p><code class="language-plaintext highlighter-rouge">conda install tensorflow-gpu=1.14.0</code></p> <p><code class="language-plaintext highlighter-rouge">conda install keras-gpu = 2.2.4</code></p> <p><code class="language-plaintext highlighter-rouge">conda install scikit-image=0.15.0</code></p> <p><code class="language-plaintext highlighter-rouge">conda install Pillow = 6.1.0</code></p> <p><code class="language-plaintext highlighter-rouge">conda install fastai = 1.0.54</code></p> <p><code class="language-plaintext highlighter-rouge">conda install pytorch=1.1.0</code></p> <p><code class="language-plaintext highlighter-rouge">conda install libtiff=4.0.10 ‚Äìno-deps</code></p> <p><code class="language-plaintext highlighter-rouge">proswap deeplearning_arcgispro</code></p> <p>Next time you‚Äôll run ArcGIS Pro, click on Python in the opening window and click on Manage Environments. You‚Äôll notice that the software has switched its active environment to your created environment, i.e., <code class="language-plaintext highlighter-rouge">deeplearning_arcgispro</code></p> <p><strong>b. ArcGIS Pro (2.6v)</strong></p> <p>Everything remains the same except the package versions. Follow everything except a few changes when typing the commands, so instead use,</p> <p>conda install tensorflow-gpu=2.1.0</p> <p>conda install keras-gpu = 2.3.1</p> <p>conda install scikit-image=0.17.2</p> <p>conda install pillow-simd=7.0.0</p> <p>conda install fastai=1.0.60</p> <p>conda install pytorch=1.4.0</p> <p>conda install libtiff=4.0.10</p> <p>conda install torchvision=0.5.0</p> <h3 id="2-creating-labels-and-exporting-data-for-deep-learning">2. Creating labels and exporting data for Deep Learning</h3> <p>This is the hardest and most time-consuming part of using Deep Learning in ArcGIS Pro. But if done sincerely and with patience can yield a good model. Now you‚Äôre going to manually create datasets for training and validation purpose. Always remember, the higher the datasets the better the model predicts or detects objects of interest.</p> <p>Begin with adding an imagery in ArcGIS Pro. Add an RGB imagery (can be a multispectral imagery with NIR &amp; RedEdge Bands too but I haven‚Äôt worked on it yet). After you have successfully added the imagery,</p> <ol> <li> <p>Click on Imagery tab and click on Classification Tools and finally click on Label Objects for Deep Learning.</p> </li> <li> <p>Once you click it, a new side window opens with Image Classification Specifications and new schema. Right click on new schema and click edit properties. Under edit properties add a class name (usually what you want the machine to detect for you). Once done, save it! You‚Äôll see that the newly created Schema shows up on the screen within the side bar. Right click on that named schema and ‚ÄúAdd a class‚Äù. This is basically creating images for different class types. Give it a name of the object you want to detect, give a value (usually 1) and color of your choice. Click on OK.</p> </li> <li> <p>Now you‚Äôll see different set of tools above your created class, click on one of those according to your choice. It can be even hand-free for object delineation.</p> </li> <li> <p>After this step, edit objects (by hand) which you want your model to detect it for you. Carefully try to collect as much data as possible. Again, the datasets should be huge to build a good model.</p> </li> <li> <p>Within the Image Classification side bar, you‚Äôll see the classes being created along with the pixel percent. After you have finished editing the objects, click on save (middle purple floppy) button. Under projects, click folders, click whatever name you have used to save the project and inside this give a feature class name.</p> </li> <li> <p>Once that is done, click on Export Training Data beside Labeled Objects in the same Image Classification sidebar,</p> </li> </ol> <p>Output Folder: Browse to the same Projects/Folders/<Name of="" your="" project="">/ImageChips (create this folder).</Name></p> <p>Image Format: JPEG (if you‚Äôre writing a code in Python, this is what the file type that the code will accept. I remember giving .tiff once and it threw an error stating that the parameters are not valid).</p> <p>Tile Size X: 256</p> <p>Tile Size Y: 256</p> <p>Stride X: 128</p> <p>Stride Y: 128</p> <p>Rotation Angle: 0 (you can change if you want)</p> <p>Meta Data Format: PASCAL Visual Object Classes (specifically for object detection)</p> <ol> <li>Run it! If you get an error here, there are probably 3 reasons,</li> </ol> <p>a. Either the versions of packages been installed are not appropriate, and the environment created, (this one is very very common issue),</p> <p>b. Problem with Output Folder specification (always use a newly made folder), or,</p> <p>c. Image Format.</p> <p>Alternatively use command line interface in Jupyter to Export your data</p> <p>https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/export-training-data-for-deelearning.htm</p> <h3 id="3-training-the-exported-data-to-build-a-model">3. Training the exported data to build a model</h3> <p>Now, ArcGIS Pro exports several files along with Images of your object of interest under ImageChips folder you made before. One of the files most important for performing Deep Learning is the .emd (ESRI Model Definition) file. This file is a passage that connects ArcGIS Pro and Deep Learning. You can even choose to edit this file and use TensorFlow, Keras according to you need and work. I‚Äôm planning in my next blog to write about how to edit these files and perform deep learning. Once you have the folder with you, you can choose to train your model either in the ArcGIS Pro Geoprocessing Tool (by typing Train Deep Learning Model) or Python. I did it in Python just to learn and visualize the interface during learning and prediction time. Below is my attached screenshot while training the data in Jupyter.</p> <p>Fig.2. Training data for object detection in ArcGIS Pro (v 2.5) If you‚Äôre using Geoprocessing tab (by clicking on Train Deep Learning Model tool, Image Analyst) in ArcGIS Pro to build a model, you can populate the required fields as follows,</p> <p>Input Training Data ‚Äî You‚Äôll add the ImageChips folder here which contains the images and .emd file as I described above</p> <p>Output Model ‚Äî Make an empty folder and name it as per your choice</p> <p>Max Epochs ‚Äî Default is 20 but I would recommend if you need a good accuracy go for a higher number, let‚Äôs say, 100. This has a direct connection with your GPU type you‚Äôre choosing. If it‚Äôs a powerful GPU, it won‚Äôt take much time.</p> <p>Model Parameters:</p> <p>Model Type: SSD (or RETINET for object detection). Don‚Äôt choose any other types as not all the models present are used for object detection.</p> <p>Batch Size: 2 (or maybe even 8, 16, 32 based on the system you‚Äôre using)</p> <p>If using SSD, specify grids [4, 2, 1], zooms [0.7, 1, 1.3] and ratios [[1, 1], [1, 0.5], [0.5, 1]] as default specifications.</p> <p>Advanced:</p> <p>Backbone Model ‚Äî ResNet 34 (or ResNet 50)</p> <p>Leave Pre-trained model as of now if you‚Äôre doing it for the first time.</p> <p>Validation ‚Äî Can be 10 or 20</p> <p>Leave everything as same and Run!</p> <p>Note: Now if you‚Äôre again getting an error, it is just because of those 3 reasons which I discussed earlier in this file. Try implementing it again. If you get all of this in one go, you‚Äôll be happy. But if not, it‚Äôs going to make you feel a lot frustrated.</p> <h3 id="4-detecting-objects-using-the-trained-model">4. Detecting objects using the trained model</h3> <p>Once everything is done successfully, all you have to do is to open ArcGIS pro again and go to Analysis -&gt; Tools -&gt; Detect Objects Using Deep Learning. You can even implement a code (as I did) just to click run and let the algorithm export a file for you with detected objects and a shape file. In ArcGIS pro, you‚Äôll see these information as you click on Detect Objects Using Deep Learning,</p> <p>Input Raster: Add your imagery here.</p> <p>Output Detected Objects: A new folder specifying where you save the shape file for the detected objects. This is really useful!</p> <p>Model Definition: Load your trained .emd file here.</p> <p>Click on Non-Maximum Suppression: This boils down a lot of detected rectangles (overlapping) to a few.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[This write up/tutorial is for those who are currently involved with working on ArcGIS Pro and want to learn a bit about Deep Learning too. Although, Deep Learning can be executed and worked independently using Python and other common platforms, I‚Äôll explain how can we integrate Deep Learning in ArcGIS Pro. If you already know how to do that, you may even choose to skip reading the write up. ArcGIS Pro has recently released 2.6 version which involves installing different newer version of Deep Learning packages within ArcGIS Pro. What needs to be noted down here is that there are several specific package versions of Deep Learning tools for ArcGIS Pro 2.5v and 2.6v. Pay attention while installing those packages because even if you miss out one package version you will end up in a lot of errors which is probably not desired to make you feel more frustrated. I have jotted down all the specific version for ArcGIS Pro 2.5v and 2.6v. Not only this but also, I have included few codes which you can write in python (just to automatize and save some time without much clicks!). Although you will find all these instructions on ESRI website (Deep Learning in ArcGIS Pro), you may have to browse through a lot of web pages back and forth to gather information from all sides. I have included all the details right here needed to integrate Deep Learning in ArcGIS Pro.]]></summary></entry><entry><title type="html">Is precision agriculture beyond the pale stream?</title><link href="https://nitin-dominic.github.io/NR/NR/blog/2020/PrecAg/" rel="alternate" type="text/html" title="Is precision agriculture beyond the pale stream?"/><published>2020-08-19T13:56:00+00:00</published><updated>2020-08-19T13:56:00+00:00</updated><id>https://nitin-dominic.github.io/NR/NR/blog/2020/PrecAg</id><content type="html" xml:base="https://nitin-dominic.github.io/NR/NR/blog/2020/PrecAg/"><![CDATA[<p>Imagine yourself in a debate about a topic you really like to talk about but then you notice the other group beating around the bush instead of addressing issues on the topic. Would you like it? Of course not, right? Looking for a precise and targeted solutions/talks about an issue is always desired no matter which area we strive for. Exploring one such area would be agriculture, where Precision Agriculture is a remedy for solving problems by being specific with an intention for a targeted applications. If this saves time, works remotely, replace traditional workflows with automation, requires researchers/users to upgrade themselves via acquainting themselves with newer skills, then may I ask you a question, why is then Precision Agriculture considered beyond the pale stream? On the contrary side, I very much agree with the fact that none of the technologies created so far are ideal (or have no disadvantages at all) and can never surpass humans in many areas and ways. But then our goal as Precision Ag. Engineers is not to develop a technology that replace humans (as in eliminating them) but assist and save time usually for those traditional methods which are done manually and are considered time-taking.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/NR/assets/img/dronerobo-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/NR/assets/img/dronerobo-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/NR/assets/img/dronerobo-1400.webp"/> <img src="/NR/assets/img/dronerobo.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> DALL.E2 generated pixelated image of a drone flying on a farm. </div> <h3 id="what-is-precision-agriculture-all-about">What is Precision Agriculture all about?</h3> <p>During my undergraduate study, I came across several myths stating that Agricultural Engineering has no scope. With that in mind, I often used to ponder what my future will be? What will I do after completing my bachelor‚Äôs? All these questions have always motivated me to pursue higher education in this field to understand what potential Agricultural Engineering has in shaping one‚Äôs career. As the trend demands with reference to emerging technologies, agricultural farms have become more automated and less human-driven. That ultimately doesn‚Äôt point out to saying that we have replaced humans with the machines. Humans are still considered an epitome for identifying, counting or detecting, etc several task on field.</p> <p>Half a century ago, most of the agricultural practices were traditional and intensely conventional. But as always, thanks to computer science, mechanical and electrical fields which bought tremendous growth and evolution after the famous ‚ÄúGreen Revolution‚Äù. Today, Precision Agriculture (a subdivision of Agricultural Engineering) has emerged with all the technologies and trending methods that once seemed difficult for other field expertise to implement on agricultural farms. Who could have ever thought that drones (which were initially used for military purposes) could be used to monitor crops in large fields? This is a very constructive piece of example I have ever read about. With the advent of tractors and mechanical implements, it is easy for the farmers to till the land in less time. With the advancements in remote sensing area, it is easy to predict crop yield, soil properties, and other related elements in soil without the traditional and arduous laboratory analysis. With the help of sensors when coupled with UAVs, plant stress or soil moisture can be monitored and estimated using the acquired imagery. With the advent of IoT, data transfer using cloud and fog computing has helped farmers knowing about various species, plant disease and other related kinds of stuff about plants on their cell phones. With the advent of UAVs or agricultural drones, it is easy to map land and extract information from UAV imagery using image processing techniques and deep learning tools. With the advent of robots it‚Äôs easy to harvest, prune, spray, manage weeds or artificially pollinate crops (Dr. Karkee; WSU) on agricultural land. In short, there‚Äôs a lot going on in the area of precision agriculture. Why not experience it yourself?</p> <h3 id="conclusion">Conclusion</h3> <p>Agricultural Engineering is a concoction of different ‚Äúengineering‚Äù fields on a field. So, therefore, is Precision Agriculture still beyond the pale stream? I mainly wrote this blog cause I had countless debates with people in the past regarding the issue of disrupting agriculture by integrating technology in it. I couldn‚Äôt do much but just to cite different works and let them know how precision agriculture is changing the face of agriculture and preparing us for future challenges. Let me know in the comments section if there are other threats and disadvantage in the area of Precision Agriculture. I‚Äôll collect information meanwhile to write a blog on the demerits of using technology in agriculture sector too.</p> <h2 id="thanks-for-your-time">Thanks for your time!</h2>]]></content><author><name></name></author><summary type="html"><![CDATA[Imagine yourself in a debate about a topic you really like to talk about but then you notice the other group beating around the bush instead of addressing issues on the topic. Would you like it? Of course not, right? Looking for a precise and targeted solutions/talks about an issue is always desired no matter which area we strive for. Exploring one such area would be agriculture, where Precision Agriculture is a remedy for solving problems by being specific with an intention for a targeted applications. If this saves time, works remotely, replace traditional workflows with automation, requires researchers/users to upgrade themselves via acquainting themselves with newer skills, then may I ask you a question, why is then Precision Agriculture considered beyond the pale stream? On the contrary side, I very much agree with the fact that none of the technologies created so far are ideal (or have no disadvantages at all) and can never surpass humans in many areas and ways. But then our goal as Precision Ag. Engineers is not to develop a technology that replace humans (as in eliminating them) but assist and save time usually for those traditional methods which are done manually and are considered time-taking.]]></summary></entry><entry><title type="html">The quest to become Homo Deus</title><link href="https://nitin-dominic.github.io/NR/NR/blog/2020/HomoDeus/" rel="alternate" type="text/html" title="The quest to become Homo Deus"/><published>2020-07-30T13:56:00+00:00</published><updated>2020-07-30T13:56:00+00:00</updated><id>https://nitin-dominic.github.io/NR/NR/blog/2020/HomoDeus</id><content type="html" xml:base="https://nitin-dominic.github.io/NR/NR/blog/2020/HomoDeus/"><![CDATA[<p><code class="language-plaintext highlighter-rouge">‚ÄúThe genie is out of the bottle. We need to move forward on artificial intelligence development but we also need to be mindful of its very real dangers. I fear that AI may replace humans altogether. If people design computer viruses, someone will design AI that replicates itself. This will be a new form of life that will outperform humans‚Äù. ~ Prof. Stephen William Hawking (Interview with Wired, November 2017)</code></p> <h3 id="what-exactly-is-the-root">What exactly is the root?</h3> <p><code class="language-plaintext highlighter-rouge">‚ÄúWe humans are blobs of organized mud, which through the impersonal workings of nature‚Äôs pattern have developed the capacity to contemplate and cherish and engage with the intimidating complexity of the world around us. The meaning we find in life is not transcendent‚Äù. ~ Sean M. Carroll (Physicist, specializing in Quantum Mechanics, gravity and cosmology, CALTECH).</code></p> <p>I was watching a 2-hour lecture given by Prof. John Lennox (Emeritus Fellow in Mathematics and Philosophy of Science, Oxford University) on ‚ÄúShould we fear Artificial Intelligence‚Äù? He described the two quintessential agendas of 21st the century which caught my attention, they are, a serious bid for immortality (where death has now become more of a technical problem) and intensification of the pursuit of happiness (in short pleasure!). He further quotes, ‚ÄúIt will be necessary to change our biochemistry and re-engineer our bodies and minds so that we shall need to re-engineer Homo Sapiens so that it can enjoy everlasting pleasure. Having raised humanity above the beastly level of survival struggles, we will now aim to upgrade into gods and turn Homo Sapiens into Homo Deus‚Äù (From the book, Homo Deus: A brief History of tomorrow by Yuval Noah Harari).</p> <h3 id="my-thought">My Thought</h3> <p>This has been a very common problem in the past. A problem that finds its grounding when humans are curiously wanting to place themselves at the seat of God. In simpler terms defining good and evil by themselves (Refer Genesis 3, you shall be as God, knowing good and evil, paraphrased). This may seem all new because it has a touch of scientism and technicality in it. The craving to know everything and achieve everything is what leads to this quest of living a life as Homo Deus.</p> <h3 id="ai-and-humans-are-different">AI and humans are different</h3> <p><code class="language-plaintext highlighter-rouge">‚ÄúIt seems to me that a lot of needless debate could be avoided if AI researchers would admit that there are fundamental differences between machine intelligence and human intelligence. Differences that cannot be overcome by any amount of research. In other words, the ‚Äúartificial‚Äù in artificial intelligence is real‚Äù. ~ Prof. Joseph Macrae Mellichamp, Emeritus Fellow of Management Sciences, University of Alabama.</code></p> <p>The assertion that the AI or ASI (Artificial superintelligence) and humans have vast differences are very true. I remember watching a movie in my 11th grade called ‚ÄúRobot‚Äù. I was seriously led into thinking that a human could possibly design a machine which could interact with the world around them with such freedom. Remember this is in 2011. Now exactly after 5 years a human robot named Sophia was activated on February 14, 2016, at South West Festival (SXSW), Texas. Possibly you can think of a future 5 years from now on. But I want to draw the reader‚Äôs attention on something else. There‚Äôs a key element that went missing in the life of that robot which is found in abundance in humans. Emotions and feelings. The plot of the movie was very hypothetical about the way the emotions were fed to the robot. The movie ended with a very drastic catastrophe caused by the robot. Possibly, it became more powerful than humans. Let‚Äôs consider several dissimilarities or uncommon characteristics between humans and robots.</p> <h3 id="emotions-and-feelings--my-thought">Emotions and Feelings | My Thought</h3> <p>We can have zero confidence that the dominant intelligence a few centuries hence will have any emotional resonance with us ‚Äì even though they may have an algorithmic understanding of the way we behaved‚Äù. ~ Prof. Martin Rees (British cosmologist and astrophysicist, University of Cambridge). May I add a little elaboration about the above-stated assertion. Though the robots may be trained to have an understanding of the way humans behave, it can never reach the way humans are made to develop feelings for other humans. For example, if I as a human is in love with someone, the emotions and feelings for that very person comes from within. No one has to program it for me in order to love the other person. But you see that‚Äôs not the case with the robots. For robots to know and love other robots or humans they have to be programmed the way humans work. And so, the robots cannot do things on their own provided it is trained by a more intellectual mind, humans.</p> <h3 id="conscious-thinking-and-intelligence--my-thought">Conscious thinking and intelligence | My Thought</h3> <p><code class="language-plaintext highlighter-rouge">‚ÄúBut here is what we are not born with: information, data, rules, software, knowledge, lexicons, representations, algorithms, programs, models, memories, images, processors, subroutines, encoders, decoders, symbols or buffers ‚Äì design elements that allow digital computers to behave somewhat intelligently. Not only are we not born with such things, we also don‚Äôt develop them ‚Äì ever. Computers, quite literally process information ‚Äì numbers, letters, words, formulas, images. Humans, on the other hand, do not ‚Äì never did, never will. It is one thing to say that brain functions like a computer. It is an entirely different thing to say that it is a computer‚Äù. ~ Roger Epstein (former editor of Psychology Today, ‚ÄúThe empty brain‚Äù).</code></p> <p>Roger Epstein has pretty much summed up the understanding between a human and a computer. Think like this. If I‚Äôm witnessing a predicament that I never ever witnessed before (completely blank, not even knowing what it means), would I be able to take the right decision or any decision? Now the answer for this question can be, yes I can take it, doesn‚Äôt matter if it‚Äôs right or wrong. But, that‚Äôs not the case with a robot. In order to take certain decision, a robot has to be programmed, in order to take certain decision or do a certain task. He cannot perform a task as we humans perform. This is what we call as ‚Äúcognitive consciousness‚Äù and ‚Äúawareness‚Äù. Robots can be intelligent (more than humans) but they are not conscious. AI separates intelligence and consciousness but remember that God links them.</p> <h3 id="final-thought">Final Thought</h3> <p><code class="language-plaintext highlighter-rouge">‚ÄúWe have paid some high prices for the technological conquest of nature, but none so high as the intellectual and spiritual costs of seeing nature as mere material for our manipulation, exploitation and transformation. With the powers of biological engineering gathering, there will be splendid new opportunities for similar degradation of our view of man. If we come to see ourselves as meat, then meat we shall become.‚Äù ~ Leon Kass (American Physician, Harvard University)</code></p> <p>I don‚Äôt have the liberty to keep on writing in lengths in this blog. But as usual final thoughts do count as they conclude the whole story about the blog. Super creation already exists. Then, who are they? The answer is simple. It‚Äôs us. Humans. Humans can and will create robots, but one thing that they will never be able to fulfil or feed in those robots, and that is their understanding of the world around them. This understanding comes from within. They can never provide an identity to the robots. They can never give different fingerprints to different robots. They can never give a different DNA to a different robot. They can never give free will to the robots. A free will to make choice between good and evil. These things sum up to one thing and that is they can never give meaning (to understand what life is all about) to a robot. Well, only God wins in the end. A God who was and is big enough to create humans in His image (Read Genesis 1:26-28). That‚Äôs exactly where life of conscious thinking and intelligence began. That‚Äôs exactly where search for a new Homo Deus is worthless.</p> <p><code class="language-plaintext highlighter-rouge">‚ÄúSuper creation already exists. God is primary, universe is derivative, Creation by speech act, ‚Äúand God said‚Äù. God speaking from an open system and not a closed natural framework‚Äù. ~ John Lennox (Emeritus Fellow in Mathematics and Philosophy of Science, Oxford University).</code></p>]]></content><author><name></name></author><summary type="html"><![CDATA[‚ÄúThe genie is out of the bottle. We need to move forward on artificial intelligence development but we also need to be mindful of its very real dangers. I fear that AI may replace humans altogether. If people design computer viruses, someone will design AI that replicates itself. This will be a new form of life that will outperform humans‚Äù. ~ Prof. Stephen William Hawking (Interview with Wired, November 2017)]]></summary></entry></feed>